{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment Questions"
      ],
      "metadata": {
        "id": "diRel0eEIb69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Simple Linear Regression?**\n",
        "\n",
        "->A method to model the relationship between a single independent variable\n",
        "𝑋\n",
        "X and a dependent variable\n",
        "𝑌\n",
        "Y using a straight line:\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "Y=mX+c\n",
        "\n",
        "**2. What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "->Linearity\n",
        "\n",
        "Independence of errors\n",
        "\n",
        "Homoscedasticity (constant variance of errors)\n",
        "\n",
        "Normality of residuals\n",
        "\n",
        "**3 . What does\n",
        "𝑚\n",
        "m represent in\n",
        "𝑌\n",
        "=\n",
        "𝑚\n",
        "𝑋\n",
        "+\n",
        "𝑐\n",
        "Y=mX+c?\n",
        "\n",
        "->The slope, indicating the change in\n",
        "𝑌\n",
        "Y for a one-unit change in\n",
        "𝑋\n",
        "X.\n",
        "\n",
        "**4.What does the intercept c represent in the equation Y=mX+c\u001d **\n",
        "\n",
        "->The intercept; the value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0.\n",
        "\n",
        "5. How do we calculate the slope\n",
        "𝑚\n",
        "m?\n",
        "\n",
        "->𝑚\n",
        "=\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        "𝑌\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "(\n",
        "∑\n",
        "𝑌\n",
        ")\n",
        "𝑛\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        "2\n",
        ")\n",
        "−\n",
        "(\n",
        "∑\n",
        "𝑋\n",
        ")\n",
        "2\n",
        "m=\n",
        "n(∑X\n",
        "2\n",
        " )−(∑X)\n",
        "2\n",
        "\n",
        "n(∑XY)−(∑X)(∑Y)\n",
        "​\n",
        "\n",
        "\n",
        "6. Purpose of least squares method:\n",
        "\n",
        "->To minimize the sum of squared differences between actual and predicted values of\n",
        "𝑌\n",
        "Y.\n",
        "\n",
        "7. How is R² interpreted?\n",
        "\n",
        "->R² represents the proportion of variance in\n",
        "𝑌\n",
        "Y explained by\n",
        "𝑋\n",
        "X. Ranges from 0 to 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "N75NXWLLIe5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "It models the relationship between two or more independent variables and one dependent variable:\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "1\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +…+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        "\n",
        "\n",
        "9. Main difference from Simple Linear Regression:\n",
        "\n",
        "Multiple inputs (independent variables) instead of one.\n",
        "\n",
        "10. Key assumptions:\n",
        "\n",
        "Same as simple regression, plus:\n",
        "\n",
        "No multicollinearity (independent variables not highly correlated)\n",
        "\n",
        "11. What is heteroscedasticity?\n",
        "\n",
        "When residual variance changes with the level of an independent variable, it can lead to inefficient estimates.\n",
        "\n",
        "12. How to handle high multicollinearity?\n",
        "\n",
        "Remove highly correlated predictors\n",
        "\n",
        "Use regularization (e.g., Ridge, Lasso)\n",
        "\n",
        "Combine variables (PCA)\n",
        "\n",
        "13. Transforming categorical variables:\n",
        "\n",
        "Use dummy/one-hot encoding, label encoding, or binary encoding.\n",
        "\n",
        "14. Role of interaction terms:\n",
        "\n",
        "They capture combined effects of variables that aren't simply additive.\n",
        "\n",
        "15. Interpretation of intercept (simple vs. multiple):\n",
        "\n",
        "Simple: Value of\n",
        "𝑌\n",
        "Y when\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0\n",
        "\n",
        "Multiple: Value of\n",
        "𝑌\n",
        "Y when all\n",
        "𝑋\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "X\n",
        "i\n",
        "​\n",
        " =0 (can be meaningless if values are not feasible)\n",
        "\n",
        "16. Significance of slope:\n",
        "\n",
        "Indicates the effect of one variable on\n",
        "𝑌\n",
        "Y, holding others constant.\n",
        "\n",
        "17. Intercept’s context:\n",
        "\n",
        "Provides baseline value of\n",
        "𝑌\n",
        "Y; useful for interpretation only if\n",
        "𝑋\n",
        "=\n",
        "0\n",
        "X=0 is meaningful.\n",
        "\n",
        "18. Limitations of R²:\n",
        "\n",
        "Doesn’t penalize for adding irrelevant predictors — adjusted R² is more reliable.\n",
        "\n",
        "19. Large standard error of coefficient:\n",
        "\n",
        "Implies high uncertainty in the coefficient estimate; could mean the variable isn't significant.\n",
        "\n",
        "20. Identifying heteroscedasticity in residual plots:\n",
        "Look for a funnel shape or pattern in residuals vs. fitted values plot.\n",
        "\n",
        "21. High R² but low adjusted R²:\n",
        "Suggests that added variables don't improve the model — possible overfitting.\n",
        "\n",
        "22. Why scale variables:\n",
        "\n",
        "To ensure all features contribute equally, especially in models with regularization.\n",
        "\n"
      ],
      "metadata": {
        "id": "f7lQlA1ikq0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23. What is polynomial regression?**\n",
        "\n",
        "A form of regression where the relationship between variables is modeled as an nth-degree polynomial.\n",
        "\n",
        "**24. How does polynomial regression differ from linear regression**\n",
        "\n",
        "Linear uses straight lines; polynomial can fit curves.\n",
        "\n",
        "**25. When is polynomial regression used?**\n",
        "\n",
        "When the data shows a nonlinear trend that a straight line can't capture.\n",
        "\n",
        "**26. General equation**:\n",
        "𝑌\n",
        "=\n",
        "𝑏\n",
        "0\n",
        "+\n",
        "𝑏\n",
        "1\n",
        "𝑋\n",
        "+\n",
        "𝑏\n",
        "2\n",
        "𝑋\n",
        "2\n",
        "+\n",
        "…\n",
        "+\n",
        "𝑏\n",
        "𝑛\n",
        "𝑋\n",
        "𝑛\n",
        "Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +…+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "\n",
        "\n",
        "**27. Can be applied to multiple variables?**\n",
        "\n",
        "Yes, known as multivariate polynomial regression.\n",
        "\n",
        "28. Limitations:\n",
        "\n",
        "Overfitting with high degrees\n",
        "\n",
        "Poor extrapolation\n",
        "\n",
        "Computationally expensive\n",
        "\n",
        "29. Model fit evaluation methods:\n",
        "\n",
        "Cross-validation\n",
        "\n",
        "Adjusted R²\n",
        "\n",
        "AIC/BIC\n",
        "\n",
        "30. Importance of visualization?\n",
        "\n",
        "Helps detect underfitting/overfitting and understand how the model captures data patterns.\n",
        "\n",
        "31. Implementation in Python:\n",
        "\n",
        "python\n",
        "\n",
        "     from sklearn.preprocessing import PolynomialFeatures\n",
        "     from sklearn.linear_model import LinearRegression\n",
        "     from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Example: 2nd-degree polynomial\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "X8rbmKAcksXb"
      }
    }
  ]
}